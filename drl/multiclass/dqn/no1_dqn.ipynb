{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9829546b457e4ca2",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7a0da379a7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = \"cpu\"\n",
    "\n",
    "if False:\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = \"cuda\"\n",
    "    elif torch.mps.is_available():\n",
    "        device_name = \"mps\"\n",
    "    # elif torch.hip.is_available():\n",
    "    #     device_name = \"hip\"\n",
    "    elif torch.mtia.is_available():\n",
    "        device_name = \"mtia\"\n",
    "    elif torch.xpu.is_available():\n",
    "        device_name = \"xpu\"\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(f\"device: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ad38480eece59",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c20eb4be516024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(r\"/Users/toshi/Documents/school/machine-learning\")\n",
    "# sys.path.append(r\"C:\\Users\\takat\\PycharmProjects\\machine-learning\")\n",
    "sys.path.append(\"/Users/toshi_pro/Documents/github-sub/machine-learning\")\n",
    "\n",
    "import flowdata\n",
    "import flowenv\n",
    "\n",
    "data, info = flowdata.flow_data.using_multiple_data()\n",
    "raw_data_train = data[0]\n",
    "raw_data_test = data[1]\n",
    "\n",
    "# train_env = gym.make(\"flowenv/FlowTrain-v0\", data=raw_data_train)\n",
    "train_env = gym.make(\"flowenv/MultiFlow-v1\", data=raw_data_train)\n",
    "# test_env = gym.make(\"flowenv/FlowTest-v0\", data=raw_data_test)\n",
    "test_env = gym.make(\"flowenv/MultiFlow-v1\", data=raw_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cd4e937dd0e7a",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2c5d2dc4692746",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Transaction = namedtuple('Transaction', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transaction(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366eff4c463bc1",
   "metadata": {},
   "source": [
    "## Plot rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986abee18b347dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards: list, show_result=False):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    # durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title(\"Result\")\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(\"Training...\")\n",
    "    means = [rewards[0]]\n",
    "    for i in range(1, len(rewards)):\n",
    "        means.append(np.mean(rewards[0:i]))\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"ratio\")\n",
    "    # plt.plot(rewards)\n",
    "    plt.plot(means, color=\"red\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8d088cc400a79",
   "metadata": {},
   "source": [
    "### Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f781b493f2dee8a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict: dict, show_result=False):\n",
    "    fig = plt.figure(figsize=(16, 20))\n",
    "\n",
    "    ac = fig.add_subplot(5, 1, 1)\n",
    "    ac.plot(metrics_dict[\"accuracy\"], label=\"accuracy\")\n",
    "    ac.grid()\n",
    "    ac.set_title(\"Accuracy\")\n",
    "\n",
    "    pr = fig.add_subplot(5, 1, 2)\n",
    "    pr.plot(metrics_dict[\"precision\"], label=\"precision\", color=\"green\")\n",
    "    pr.grid()\n",
    "    pr.set_title(\"Precision\")\n",
    "\n",
    "    re = fig.add_subplot(5, 1, 3)\n",
    "    re.plot(metrics_dict[\"recall\"], label=\"recall\", color=\"red\")\n",
    "    re.grid()\n",
    "    re.set_title(\"Recall\")\n",
    "\n",
    "    f1 = fig.add_subplot(5, 1, 4)\n",
    "    f1.plot(metrics_dict[\"f1\"], label=\"f1\", color=\"black\")\n",
    "    f1.grid()\n",
    "    f1.set_title(\"F1\")\n",
    "\n",
    "    fpr = fig.add_subplot(5, 1, 5)\n",
    "    fpr.plot(metrics_dict[\"fpr\"], label=\"fpr\", color=\"purple\")\n",
    "    fpr.grid()\n",
    "    fpr.set_title(\"FPR\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "\n",
    "def calculate_metrics(tp, tn, fp, fn):\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else -1\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else -1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall != 0 else None\n",
    "    fpr = fp / (fp + tn) if fp + tn != 0 else None\n",
    "\n",
    "    if precision < 0:\n",
    "        precision = None\n",
    "    if recall < 0:\n",
    "        recall = None\n",
    "    return accuracy, precision, recall, f1, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ae2c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_action_and_answer(episode=None, actions=None, answers=None, test=False):\n",
    "    all_data = []\n",
    "    class_name = flowdata.flow_data.label_info()\n",
    "    if test:\n",
    "        file_name = \"action_and_answer_test.csv\"\n",
    "        report_dict = classification_report(\n",
    "            answers, actions, labels=[i for i in range(len(class_name))], \n",
    "            target_names=class_name, output_dict=True, zero_division=0\n",
    "        )\n",
    "\n",
    "        for class_, metrics in report_dict.items():\n",
    "            if class_ in class_name:\n",
    "                entry = {\n",
    "                    \"episode\": episode,\n",
    "                    \"class\": class_,\n",
    "                    \"precision\": metrics[\"precision\"],\n",
    "                    \"recall\": metrics[\"recall\"],\n",
    "                    \"f1-score\": metrics[\"f1-score\"],\n",
    "                    \"support\": metrics[\"support\"]\n",
    "                }\n",
    "                all_data.append(entry)\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv(file_name, mode=\"w\", header=False)\n",
    "        return\n",
    "    \n",
    "    file_name = \"action_and_answer.csv\"\n",
    "    # y_true: answer, y_pred: action\n",
    "    for i, (y_true, y_pred) in enumerate(zip(answers, actions)):\n",
    "        report_dict = classification_report(y_true, y_pred, labels=[i for i in range(len(class_name))], target_names=class_name, output_dict=True, zero_division=0)\n",
    "        for class_, metrics in report_dict.items():\n",
    "            if class_ in class_name:\n",
    "                entry = {\n",
    "                    \"episode\": i,\n",
    "                    \"class\": class_,\n",
    "                    \"precision\": metrics[\"precision\"],\n",
    "                    \"recall\": metrics[\"recall\"],\n",
    "                    \"f1-score\": metrics[\"f1-score\"],\n",
    "                    \"support\": metrics[\"support\"]\n",
    "                }\n",
    "                all_data.append(entry)\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(file_name, mode=\"w\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a75ff736ef3f0",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195b8288b5734359",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa817816f39d09",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5412e4fc54b9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_TARGET_STEPS = 200\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 100000\n",
    "TAU = 0.005\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52e62d70492c4",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f7f8f10e264213",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "n_actions = train_env.action_space.n\n",
    "n_inputs = train_env.observation_space.shape[0]\n",
    "\n",
    "state, info = train_env.reset()\n",
    "# print(info)\n",
    "\n",
    "policy_net = DQNetwork(n_inputs, n_actions).to(device)\n",
    "target_net = DQNetwork(n_inputs, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "steps_done = 0\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "episode_rewards = []\n",
    "episode_precision = []\n",
    "episode_action = []\n",
    "episode_answer = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ad37010e273fd",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd3944eb69dc45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state_tensor: torch.Tensor):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state_tensor).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], dtype=torch.long, device=device)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transaction(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    utils.clip_grad_value_(policy_net.parameters(), 1000)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe25c872c64b32",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "881e2a53-9c47-45d2-9d8e-fc84b87e1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187feeddf30c4b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_action = []\n",
    "count_answer = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    random.seed(i_episode)\n",
    "    sum_reward = 0\n",
    "    confusion_matrix = np.zeros((2,2), dtype=int)\n",
    "\n",
    "    count_action = []\n",
    "    count_answer = []\n",
    "\n",
    "    initial_state, info = train_env.reset()\n",
    "    state = torch.tensor(initial_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        # select action\n",
    "        action = select_action(state)\n",
    "\n",
    "        # print(action)\n",
    "        # calculate next state\n",
    "        raw_next_state, reward, terminated, truncated, info = train_env.step(action.item())\n",
    "        count_action.append(info[\"action\"])\n",
    "        count_answer.append(info[\"answer\"])\n",
    "\n",
    "        row_column_index = info[\"confusion_position\"]\n",
    "        confusion_matrix[row_column_index[0], row_column_index[1]] += 1\n",
    "\n",
    "        # to tensor\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(raw_next_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        sum_reward += reward.item() if reward.item() == 1 else 0\n",
    "\n",
    "        # move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # optimize the model\n",
    "        optimize_model()\n",
    "\n",
    "        if terminated:\n",
    "            episode_rewards.append(sum_reward / (t + 1))\n",
    "            break\n",
    "\n",
    "    # do after the episode\n",
    "    episode_action.append(count_action)\n",
    "    episode_answer.append(count_answer)\n",
    "    # episode_rewards.append(sum_reward)\n",
    "    base = confusion_matrix[0, 0] + confusion_matrix[1, 0]\n",
    "    episode_precision.append(\n",
    "        confusion_matrix[0,0] / base if base != 0 else 0\n",
    "    )\n",
    "    # print(i_episode)\n",
    "    if i_episode > 0 and i_episode % 10 == 0:\n",
    "        plot_rewards(episode_precision)\n",
    "\n",
    "# complete the episode\n",
    "write_action_and_answer(actions=episode_action, answers=episode_answer)\n",
    "plot_rewards(episode_precision, show_result=True)\n",
    "torch.save(policy_net.state_dict(), \"dqn_no2.pth\")  # save the model\n",
    "\n",
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081e0bb-5c1a-4b6a-ac22-7836d305d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(episode_rewards, show_result=True)\n",
    "torch.save(policy_net.state_dict(), \"dqn_no2.pth\")  # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef7b76-3f21-4a40-95a5-d73130c69c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(episode_precision, show_result=True)\n",
    "write_action_and_answer(actions=episode_action, answers=episode_answer)\n",
    "# print(episode_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9740beca7384b44",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4d16a6f258368",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"dqn_no2.pth\"\n",
    "\n",
    "# load the model\n",
    "trained_network = DQNetwork(n_inputs=n_inputs, n_outputs=n_actions).to(device)\n",
    "trained_network.load_state_dict(torch.load(MODEL_PATH, map_location=device, weights_only=True))\n",
    "trained_network.eval()\n",
    "\n",
    "# test the model\n",
    "\n",
    "confusion_array = np.zeros((2, 2), dtype=np.int32)\n",
    "metrics_dictionary = {\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": [],\n",
    "    \"fpr\": []\n",
    "}\n",
    "\n",
    "count_action = []\n",
    "count_answer = []\n",
    "\n",
    "for i_loop in range(1000):\n",
    "    random.seed(i_loop)\n",
    "    test_raw_state, _ = test_env.reset()\n",
    "    test_state = torch.tensor(test_raw_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "    for t in count():\n",
    "        with torch.no_grad():\n",
    "            test_action = trained_network(test_state).max(1).indices.view(1, 1)\n",
    "\n",
    "        test_raw_next_state, test_reward, test_terminated, test_truncated, test_info = test_env.step(test_action.item())\n",
    "\n",
    "        # calculate confusion matrix\n",
    "        raw = 0 if test_reward == 1 else 1\n",
    "\n",
    "        count_action.append(test_info[\"action\"])\n",
    "        count_answer.append(int(test_info[\"answer\"]))\n",
    "\n",
    "        # test_info = (row, column) means confusion matrix index\n",
    "        index = test_info[\"confusion_position\"]\n",
    "        confusion_array[index[0], index[1]] += 1\n",
    "\n",
    "        if test_terminated:\n",
    "            break\n",
    "\n",
    "        # make next state tensor and update state\n",
    "        test_state = torch.tensor(test_raw_next_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # calculate metrics\n",
    "    tp = confusion_array[0, 0]\n",
    "    tn = confusion_array[1, 1]\n",
    "    fp = confusion_array[0, 1]\n",
    "    fn = confusion_array[1, 0]\n",
    "    print(i_loop, tp, tn, fp, fn)\n",
    "\n",
    "    accuracy, precision, recall, f1, fpr = calculate_metrics(tp, tn, fp, fn)\n",
    "    metrics_dictionary[\"accuracy\"].append(accuracy)\n",
    "    metrics_dictionary[\"precision\"].append(precision)\n",
    "    metrics_dictionary[\"recall\"].append(recall)\n",
    "    metrics_dictionary[\"f1\"].append(f1)\n",
    "    metrics_dictionary[\"fpr\"].append(fpr)\n",
    "    # print(accuracy, precision, recall, f1, fpr)\n",
    "\n",
    "    \n",
    "\n",
    "    if i_loop % 50 == 0:\n",
    "        plot_metrics(metrics_dictionary)\n",
    "\n",
    "print(count_action)\n",
    "print(count_answer)\n",
    "write_action_and_answer(actions=count_action, answers=count_answer, test=True)\n",
    "# plot metrics\n",
    "plot_metrics(metrics_dictionary, show_result=True)\n",
    "print(f\" accuracy: {metrics_dictionary['accuracy'][-1]}\")\n",
    "print(f\"precision: {metrics_dictionary['precision'][-1]}\")\n",
    "print(f\"  recall : {metrics_dictionary['recall'][-1]}\")\n",
    "print(f\"    f1   : {metrics_dictionary['f1'][-1]}\")\n",
    "print(f\"   fpr   : {metrics_dictionary['fpr'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f27731a-17ce-4beb-a4cb-5ce40122caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_action_and_answer(episode=1000, actions=count_action, answers=count_answer, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e81e31-af3b-4efb-ab2a-54f7f22729ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3345b5d-b4a3-4d5c-a70f-70106f10b995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
