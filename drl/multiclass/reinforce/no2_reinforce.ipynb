{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9829546b457e4ca2",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "from time import time\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.vector import AsyncVectorEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a7a0da379a7907",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device_name = \"cpu\"\n",
    "\n",
    "if False:\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = \"cuda\"\n",
    "    elif torch.mps.is_available():\n",
    "        device_name = \"mps\"\n",
    "    # elif torch.hip.is_available():\n",
    "    #     device_name = \"hip\"\n",
    "    elif torch.mtia.is_available():\n",
    "        device_name = \"mtia\"\n",
    "    elif torch.xpu.is_available():\n",
    "        device_name = \"xpu\"\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(f\"device: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ad38480eece59",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acfd1298-1472-4c80-a7cf-f2fd3ca4617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"/Users/toshi_pro/Documents/github-sub/machine-learning\")\n",
    "# sys.path.append(\"/Users/toshi/Documents/school/machine-learning\")\n",
    "sys.path.append(r\"C:\\Users\\takat\\PycharmProjects\\machine-learning\")\n",
    "import flowdata\n",
    "import flowenv\n",
    "\n",
    "def make_env(phase=\"train\"):\n",
    "    def _init():\n",
    "        data, info = flowdata.flow_data.using_multiple_data()\n",
    "        raw_data_train = data[0]\n",
    "        raw_data_test = data[1]\n",
    "        if phase == \"train\":\n",
    "            return gym.make(\"flowenv/MultiFlow-v1\", data=raw_data_train)\n",
    "        else:\n",
    "            return gym.make(\"flowenv/MultiFlow-v1\", data=raw_data_test)\n",
    "    return _init\n",
    "\n",
    "NUM_ENVS = 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2671b548-d5ca-43e9-b5bf-4b4a0ba3f6fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[WinError 232] パイプを閉じています。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_envs \u001b[38;5;241m=\u001b[39m \u001b[43mAsyncVectorEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUM_ENVS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_env \u001b[38;5;241m=\u001b[39m make_env(phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)()\n",
      "File \u001b[1;32mc:\\Users\\takat\\PycharmProjects\\machine-learning\\.venv\\Lib\\site-packages\\gymnasium\\vector\\async_vector_env.py:225\u001b[0m, in \u001b[0;36mAsyncVectorEnv.__init__\u001b[1;34m(self, env_fns, shared_memory, copy, context, daemon, worker, observation_mode)\u001b[0m\n\u001b[0;32m    222\u001b[0m         child_pipe\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m AsyncState\u001b[38;5;241m.\u001b[39mDEFAULT\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\takat\\PycharmProjects\\machine-learning\\.venv\\Lib\\site-packages\\gymnasium\\vector\\async_vector_env.py:619\u001b[0m, in \u001b[0;36mAsyncVectorEnv._check_spaces\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_is_running()\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent_pipes:\n\u001b[1;32m--> 619\u001b[0m     \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_check_spaces\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_observation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_action_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m results, successes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[pipe\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent_pipes])\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_errors(successes)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\multiprocessing\\connection.py:205\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\multiprocessing\\connection.py:279\u001b[0m, in \u001b[0;36mPipeConnection._send_bytes\u001b[1;34m(self, buf)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, buf):\n\u001b[1;32m--> 279\u001b[0m     ov, err \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWriteFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mERROR_IO_PENDING:\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [WinError 232] パイプを閉じています。"
     ]
    }
   ],
   "source": [
    "train_envs = AsyncVectorEnv([make_env() for _ in range(NUM_ENVS)])\n",
    "test_env = make_env(phase=\"test\")()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cd4e937dd0e7a",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2c5d2dc4692746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transaction = namedtuple('Transaction', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transaction(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "Trajectory = namedtuple('Trajectory', (\"rewards\", \"log_probs\"))\n",
    "\n",
    "class EpisodeMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Trajectory(*args))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    # last batch_size memory output\n",
    "    def sample(self, batch_size):\n",
    "        return list(self.memory)[-batch_size:]\n",
    "\n",
    "    def sample_random(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366eff4c463bc1",
   "metadata": {},
   "source": [
    "## Plot rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986abee18b347dab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(accuracy_list: list, show_result=False):\n",
    "    if len(accuracy_list) < 1:\n",
    "        return\n",
    "    plt.figure(1)\n",
    "    # durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    maximum = np.max(accuracy_list)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title(\"Result\")\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f\"Training... max:{maximum:.3}\")\n",
    "    means = [accuracy_list[0]]\n",
    "    for i in range(1, len(accuracy_list)):\n",
    "        if i > 100:\n",
    "            means.append(np.mean(accuracy_list[i-100:i]))\n",
    "        else:\n",
    "            means.append(np.mean(accuracy_list[0:i]))\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlim(len(accuracy_list)-900, len(accuracy_list)+100)\n",
    "    # plt.plot(rewards)\n",
    "    plt.plot(means, color=\"red\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8d088cc400a79",
   "metadata": {},
   "source": [
    "### Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f781b493f2dee8a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict: dict, show_result=False):\n",
    "    display.clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "    ac = fig.add_subplot(3, 2, 1)\n",
    "    ac.plot(metrics_dict[\"accuracy\"], label=\"accuracy\")\n",
    "    ac.grid()\n",
    "    ac.set_title(\"Accuracy\")\n",
    "\n",
    "    pr = fig.add_subplot(3, 2, 2)\n",
    "    pr.plot(metrics_dict[\"precision\"], label=\"precision\", color=\"green\")\n",
    "    pr.grid()\n",
    "    pr.set_title(\"Precision\")\n",
    "\n",
    "    re = fig.add_subplot(3, 2, 3)\n",
    "    re.plot(metrics_dict[\"recall\"], label=\"recall\", color=\"red\")\n",
    "    re.grid()\n",
    "    re.set_title(\"Recall\")\n",
    "\n",
    "    f1 = fig.add_subplot(3, 2, 4)\n",
    "    f1.plot(metrics_dict[\"f1\"], label=\"f1\", color=\"black\")\n",
    "    f1.grid()\n",
    "    f1.set_title(\"F1\")\n",
    "\n",
    "    fpr = fig.add_subplot(3, 2, 5)\n",
    "    fpr.plot(metrics_dict[\"fpr\"], label=\"fpr\", color=\"purple\")\n",
    "    fpr.grid()\n",
    "    fpr.set_title(\"FPR\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "\n",
    "def calculate_metrics(tp, tn, fp, fn):\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else -1\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else -1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if fp + tn != 0 else 0.0\n",
    "\n",
    "    if precision < 0:\n",
    "        precision = 0.0\n",
    "    if recall < 0:\n",
    "        recall = 0.0\n",
    "    return accuracy, precision, recall, f1, fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a75ff736ef3f0",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a004798cb2fc841",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.common_fc = nn.Sequential(\n",
    "            nn.Linear(n_inputs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.probs = nn.Sequential(\n",
    "            nn.Linear(128, n_outputs),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.common_fc(x)\n",
    "        probs = self.probs(x)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ee49e58d3557a",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1fa23955fd1c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52e62d70492c4",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45fb7bba5e7d4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "\n",
    "n_inputs = train_envs.single_observation_space.shape[0]\n",
    "n_outputs = train_envs.single_action_space.n\n",
    "\n",
    "policy_net = PolicyNetwork(n_inputs, n_outputs).to(device)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "\n",
    "steps_done = 0\n",
    "memory = ReplayMemory(1000000)\n",
    "episode_memory = EpisodeMemory(100000)\n",
    "returns = []\n",
    "episode_accuracy = []\n",
    "episode_metrics = {\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": [],\n",
    "    \"fpr\": []\n",
    "}\n",
    "episode_rewards = [[] for _ in range(NUM_ENVS)]\n",
    "episode_log_probs = [[] for _ in range(NUM_ENVS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ad37010e273fd",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f71fba0aad48cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(states):\n",
    "    states = states.clone().detach().requires_grad_(True)\n",
    "    probs = policy_net(states)\n",
    "\n",
    "    distributions = torch.distributions.Categorical(probs)\n",
    "    actions = distributions.sample()\n",
    "    log_probs = distributions.log_prob(actions)\n",
    "\n",
    "    return actions, log_probs\n",
    "\n",
    "def calculate_returns(rewards):\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    G = 0\n",
    "    try:\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            G = rewards[i] + GAMMA * G\n",
    "            returns[i] = G\n",
    "    except:\n",
    "        returns[0] = rewards[0]\n",
    "    return returns.clone().detach().requires_grad_(True)\n",
    "\n",
    "def optimize_model():\n",
    "    # print(log_probs)\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    # trajectory = episode_memory.sample(BATCH_SIZE)\n",
    "    trajectory = episode_memory.sample_random(BATCH_SIZE)\n",
    "    batch = Trajectory(*zip(*trajectory))\n",
    "\n",
    "    rewards = torch.cat(batch.rewards).squeeze()\n",
    "    log_probs = torch.cat(batch.log_probs).squeeze()\n",
    "\n",
    "    returns = calculate_returns(rewards)\n",
    "    baseline = returns.mean()\n",
    "    advantage = returns - baseline\n",
    "\n",
    "    loss = -(log_probs * advantage).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b2384c576de98",
   "metadata": {},
   "source": [
    "REINFORCE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcffe90ed4225c6",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8a1d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_action_and_answer(episode=None, action=None, answer=None, first=False, test=False):\n",
    "    if test:\n",
    "        file_name = \"action_and_answer_test.csv\"\n",
    "    else:\n",
    "        file_name = \"action_and_answer.csv\"\n",
    "    if first:\n",
    "        labels = flowdata.flow_data.label_info()\n",
    "        presets = [\"action\", \"answer\"]\n",
    "        f = open(file_name, \"w\")\n",
    "        f.write(\"time,episode,action_sum,\")\n",
    "        for preset in presets:\n",
    "            for label in labels:\n",
    "                f.write(f\"{preset}_{label},\")\n",
    "        else:\n",
    "            f.write(\"\\n\")\n",
    "        return\n",
    "    \n",
    "    dt_now = datetime.datetime.now()\n",
    "    f = open(file_name, \"a\")\n",
    "    sum_action = sum(action)\n",
    "    f.write(f\"{dt_now.isoformat()},{episode},{sum_action},\")\n",
    "    action_and_answer = action + answer\n",
    "\n",
    "    for count_num in action_and_answer:\n",
    "        f.write(f\"{count_num},\")\n",
    "    else:\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f4d16a6f258368",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    MODEL_PATH = \"no4_reinforce.pth\"\n",
    "    write_action_and_answer(first=True, test=True)\n",
    "\n",
    "    # load the model\n",
    "    trained_network = PolicyNetwork(n_inputs, n_outputs).to(device)\n",
    "    trained_network.load_state_dict(torch.load(MODEL_PATH, map_location=device, weights_only=True))\n",
    "    trained_network.eval()\n",
    "\n",
    "    # test the model\n",
    "\n",
    "    confusion_array = np.zeros((2, 2), dtype=np.int32)\n",
    "    metrics_dictionary = {\n",
    "        \"accuracy\": [],\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1\": [],\n",
    "        \"fpr\": []\n",
    "    }\n",
    "\n",
    "    count_action = [0 for _ in range(n_outputs)]\n",
    "    count_answer = [0 for _ in range(n_outputs)]\n",
    "\n",
    "    for i_loop in range(100):\n",
    "        test_raw_state, _ = test_env.reset()\n",
    "        test_state = torch.tensor(test_raw_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        for t in count():\n",
    "            with torch.no_grad():\n",
    "                prob_distribution = trained_network(test_state)\n",
    "                test_action = torch.multinomial(prob_distribution, 1)\n",
    "\n",
    "            test_raw_next_state, test_reward, test_terminated, test_truncated, test_info = test_env.step(test_action.item())\n",
    "\n",
    "            # calculate confusion matrix\n",
    "            raw = 0 if test_reward == 1 else 1\n",
    "\n",
    "            count_action[test_info[\"action\"]] += 1\n",
    "            count_answer[test_info[\"answer\"]] += 1\n",
    "\n",
    "            # test_info = (row, column) means confusion matrix index\n",
    "            index = test_info[\"confusion_position\"]\n",
    "            confusion_array[index[0], index[1]] += 1\n",
    "\n",
    "            if test_terminated:\n",
    "                break\n",
    "\n",
    "            # make next state tensor and update state\n",
    "            test_state = torch.tensor(test_raw_next_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        write_action_and_answer(episode=i_loop, action=count_action, answer=count_answer, test=True)\n",
    "\n",
    "        # calculate metrics\n",
    "        tp = confusion_array[0, 0]\n",
    "        tn = confusion_array[1, 1]\n",
    "        fp = confusion_array[0, 1]\n",
    "        fn = confusion_array[1, 0]\n",
    "\n",
    "        accuracy, precision, recall, f1, fpr = calculate_metrics(tp, tn, fp, fn)\n",
    "        metrics_dictionary[\"accuracy\"].append(accuracy)\n",
    "        metrics_dictionary[\"precision\"].append(precision)\n",
    "        metrics_dictionary[\"recall\"].append(recall)\n",
    "        metrics_dictionary[\"f1\"].append(f1)\n",
    "        metrics_dictionary[\"fpr\"].append(fpr)\n",
    "        # print(tp, tn, fp, tn)\n",
    "\n",
    "    return [np.mean(metrics_dictionary[\"accuracy\"]), np.mean(metrics_dictionary[\"precision\"]), np.mean(metrics_dictionary[\"recall\"]), np.mean(metrics_dictionary[\"f1\"]), np.mean(metrics_dictionary[\"fpr\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bad6848-1ef4-497a-9f97-3e65c1fab15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9804d54-fc37-473d-8b3c-39bbfa2fbe17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((2,2), dtype=int)\n",
    "sum_reward = 0\n",
    "# print(i_episode)\n",
    "\n",
    "seeds = [random.randint(0, 100000) for _ in range(NUM_ENVS)]\n",
    "#print(seeds)\n",
    "initial_states, info = train_envs.reset(seed=seeds)\n",
    "states = torch.tensor(initial_states, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "write_action_and_answer(first=True)\n",
    "\n",
    "count_action = [0 for _ in range(n_outputs)]\n",
    "count_answer = [0 for _ in range(n_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ca6c0f-aaaf-4798-bbe6-69f10c6bfaef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m next_states, rewards, terminated, truncated, info \u001b[38;5;241m=\u001b[39m train_envs\u001b[38;5;241m.\u001b[39mstep_wait()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# info[\"action\"]: action, info[\"answer\"]: answer\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mcount_action\u001b[49m\u001b[43m[\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m count_answer[info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# calculate confusion matrix\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "for t in range(num_steps):\n",
    "    # Select and perform an action\n",
    "    actions, log_probs = select_action(states)\n",
    "    actions_np = actions.cpu().numpy()[0]\n",
    "    train_envs.step_async(actions_np)\n",
    "\n",
    "    next_states, rewards, terminated, truncated, info = train_envs.step_wait()\n",
    "\n",
    "    # info[\"action\"]: action, info[\"answer\"]: answer\n",
    "    for num in info[\"action\"]:\n",
    "        count_action[num] += 1\n",
    "    for num in info[\"answer\"]:\n",
    "        count_answer[num] += 1\n",
    "\n",
    "    # calculate confusion matrix\n",
    "    for item in info[\"confusion_position\"]:\n",
    "        confusion_matrix[item[0], item[1]] += 1\n",
    "\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
    "    for i in range(NUM_ENVS):\n",
    "        episode_rewards[i].append(rewards[i])\n",
    "\n",
    "        if terminated[i] or truncated[i]:\n",
    "            buf_rewards = torch.tensor(episode_rewards[i], dtype=torch.float32)\n",
    "            buf_log_probs = torch.tensor(episode_log_probs[i], dtype=torch.float32)\n",
    "            episode_memory.push(buf_rewards, buf_log_probs)\n",
    "\n",
    "            write_action_and_answer(episode=len(episode_memory),action=count_action, answer=count_answer)\n",
    "            \n",
    "            count_action = [0 for _ in range(n_outputs)]\n",
    "            count_answer = [0 for _ in range(n_outputs)]\n",
    "\n",
    "            episode_rewards = [[] for _ in range(NUM_ENVS)]\n",
    "            episode_log_probs = [[] for _ in range(NUM_ENVS)]\n",
    "            next_states, _ = train_envs.reset(seed=random.randint(0, 1000))\n",
    "            accuracy = (confusion_matrix[0, 0] + confusion_matrix[1, 1]) / confusion_matrix.sum()\n",
    "            episode_accuracy.append(accuracy)\n",
    "            optimize_model()\n",
    "\n",
    "    \n",
    "    # to tensor\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    states = next_states\n",
    "\n",
    "    # not reuse graph\n",
    "    if t > 0:\n",
    "        if t % 25 == 0:\n",
    "            plot_accuracy(episode_accuracy)\n",
    "        if t % 5000 == 0:\n",
    "            print(t)\n",
    "            torch.save(policy_net.state_dict(), \"no4_reinforce.pth\")\n",
    "            ac, pr, re, f1, fp = test()\n",
    "            with open(f\"metrics_{BATCH_SIZE}.csv\", \"a\") as f:\n",
    "                f.write(f\"{t},{ac},{pr},{re},{f1},{fp}\\n\")\n",
    "    #print(t)\n",
    "\n",
    "else:\n",
    "    torch.save(policy_net.state_dict(), \"no4_reinforce.pth\")  # save the model\n",
    "\"\"\"\n",
    "    if i_episode > 0 and i_episode % 100 == 0:\n",
    "        torch.save(policy_net.state_dict(), \"no3_reinforce.pth\")  # save the model\n",
    "\n",
    "        print(f\"Episode {i_episode}: {sum_reward}\")\n",
    "        ac, pr, re, f1, fp = test()\n",
    "        episode_metrics[\"accuracy\"].append(ac)\n",
    "        episode_metrics[\"precision\"].append(pr)\n",
    "        episode_metrics[\"recall\"].append(re)\n",
    "        episode_metrics[\"f1\"].append(f1)\n",
    "        episode_metrics[\"fpr\"].append(fp)\n",
    "        plot_metrics(episode_metrics)\n",
    "\"\"\"\n",
    "\n",
    "# complete the episode\n",
    "# plot_metrics(episode_metrics, show_result=True)\n",
    "plot_accuracy(episode_accuracy, show_result=True)\n",
    "\n",
    "# train_envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6864d-8fb3-47be-952d-8a2cac4a79c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(f\"Episode {i_episode}: {sum_reward}\")\n",
    "for i in range(5):\n",
    "    ac, pr, re, f1, fp = test()\n",
    "    episode_metrics[\"accuracy\"].append(ac)\n",
    "    episode_metrics[\"precision\"].append(pr)\n",
    "    episode_metrics[\"recall\"].append(re)\n",
    "    episode_metrics[\"f1\"].append(f1)\n",
    "    episode_metrics[\"fpr\"].append(fp)\n",
    "    print(i)\n",
    "plot_metrics(episode_metrics, show_result=True)\n",
    "print(ac, pr, re, f1, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9740beca7384b44",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
