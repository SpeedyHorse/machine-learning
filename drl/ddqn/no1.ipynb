{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DDQN",
   "id": "7a3a434b11a18d6"
  },
  {
   "cell_type": "code",
   "id": "ed77a56cece86c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T09:59:35.368807Z",
     "start_time": "2024-12-31T09:59:33.484570Z"
    }
   },
   "source": [
    "import matplotlib\n",
    "import random\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "from time import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T09:59:35.397982Z",
     "start_time": "2024-12-31T09:59:35.370811Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "a8b21536077fd189",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T09:59:35.431749Z",
     "start_time": "2024-12-31T09:59:35.428277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device_name = \"cpu\"\n",
    "\n",
    "if True:\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = \"cuda\"\n",
    "    elif torch.mps.is_available():\n",
    "        device_name = \"mps\"\n",
    "    # elif torch.hip.is_available():\n",
    "    #     device_name = \"hip\"\n",
    "    elif torch.mtia.is_available():\n",
    "        device_name = \"mtia\"\n",
    "    elif torch.xpu.is_available():\n",
    "        device_name = \"xpu\"\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(f\"device: {device_name}\")"
   ],
   "id": "5fc9531282d0e7be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T09:59:35.443708Z",
     "start_time": "2024-12-31T09:59:35.440701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "UPDATE_TARGET_STEPS = 500\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000000\n",
    "TAU = 0.005\n",
    "LR = 1e-4"
   ],
   "id": "4eb1683094c43a39",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T09:59:35.460975Z",
     "start_time": "2024-12-31T09:59:35.452551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\takat\\PycharmProjects\\machine-learning\")\n",
    "import flowdata\n",
    "import flowenv"
   ],
   "id": "cb36951b35f7d0c7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-31T09:59:35.590899Z",
     "start_time": "2024-12-31T09:59:35.471688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from flowdata.src.flow_data import using_data\n",
    "\n",
    "raw_data_train, raw_data_test = flowdata.flow_data.using_data()\n",
    "\n",
    "train_env = gym.make(\"flowenv/FlowTrain-v0\", data=raw_data_train)\n",
    "test_env = gym.make(\"flowenv/FlowTest-v0\", data=raw_data_test)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T09:59:35.604986Z",
     "start_time": "2024-12-31T09:59:35.601787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Transaction = namedtuple('Transaction', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transaction(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "id": "7024a28b6dfdb7f5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T09:59:35.618808Z",
     "start_time": "2024-12-31T09:59:35.616044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ],
   "id": "9d76cc101b28b0de",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T09:59:35.635998Z",
     "start_time": "2024-12-31T09:59:35.630597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def select_action(state):\n",
    "    global n_actions, policy_net, steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], dtype=torch.long, device=device)\n",
    "\n",
    "def optimize_model():\n",
    "    global policy_net, target_net, optimizer, memory\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transaction(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(non_final_next_states).max(1).indices.unsqueeze(1)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, next_actions).squeeze(1)\n",
    "\n",
    "    expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "def get_h_m_s(seconds: float):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds - hours * 3600) // 60)\n",
    "    seconds = seconds - hours * 3600 - minutes * 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "def loading_bar(episode, total_episodes, interval):\n",
    "    pro_size_float = (episode + 1) / total_episodes * 20\n",
    "    show = pro_size_float * 5\n",
    "    pro_size = int(pro_size_float)\n",
    "\n",
    "    # episode...interval -> total_episodes...interval * total_episodes / episode\n",
    "    last_time = interval * (total_episodes - episode) / (episode + 1)\n",
    "    hours, minutes, seconds = get_h_m_s(last_time)\n",
    "    print(f\"\\r[{'#' * pro_size}{' ' * (20 - pro_size)}] {show:3.02f}%, last={hours:02d}:{minutes:02d}:{seconds:03.3f}\", end=\"\")"
   ],
   "id": "5bfbcb48af51e6e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-31T09:59:35.646435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_actions = train_env.action_space.n\n",
    "n_inputs = train_env.observation_space.shape[0]\n",
    "\n",
    "state, info = train_env.reset()\n",
    "\n",
    "policy_net = DQN(n_inputs, n_actions).to(device)\n",
    "target_net = DQN(n_inputs, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(int(1e6))\n",
    "\n",
    "num_episodes = 10000\n",
    "steps_done = 0\n",
    "episode_rewards = []\n",
    "mean_rewards = []\n",
    "\n",
    "x = []\n",
    "\n",
    "start_time = time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = train_env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    sum_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = train_env.step(action.item())\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = bool(terminated)\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "        sum_reward += reward.item()\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if steps_done % UPDATE_TARGET_STEPS == 0:\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in target_net_state_dict:\n",
    "                target_net_state_dict[key] = TAU * policy_net_state_dict[key] + (1 - TAU) * target_net_state_dict[key]\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(sum_reward)\n",
    "            mean_rewards.append(np.mean(episode_rewards))\n",
    "            break\n",
    "    x.append(i_episode)\n",
    "    end_time = time()\n",
    "\n",
    "    if i_episode > 0 and i_episode % 100 == 0:\n",
    "        print(f\" Episode {i_episode}, reward: {sum_reward}\")\n",
    "\n",
    "    loading_bar(i_episode, num_episodes, end_time - start_time)\n",
    "\n",
    "plt.plot(episode_rewards)\n",
    "plt.plot(mean_rewards, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "result = pd.DataFrame(episode_rewards, columns=[\"reward\"])\n",
    "torch.save(policy_net.state_dict(), \"ddqn.pth\")\n",
    "train_env.close()"
   ],
   "id": "ad35307778263903",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                    ] 1.00%, last=01:27:27.170 Episode 100, reward: 16\n",
      "[                    ] 2.00%, last=01:27:33.330 Episode 200, reward: 12\n",
      "[                    ] 3.00%, last=01:27:9.3073 Episode 300, reward: 4\n",
      "[                    ] 4.00%, last=01:26:27.363 Episode 400, reward: 22\n",
      "[#                   ] 5.00%, last=01:26:3.6689 Episode 500, reward: 10\n",
      "[#                   ] 6.00%, last=01:25:36.058 Episode 600, reward: 10\n",
      "[#                   ] 7.00%, last=01:24:51.765 Episode 700, reward: 4\n",
      "[#                   ] 8.00%, last=01:24:19.680 Episode 800, reward: 22\n",
      "[#                   ] 9.00%, last=01:23:53.786 Episode 900, reward: 10\n",
      "[##                  ] 10.00%, last=01:23:23.886 Episode 1000, reward: 8\n",
      "[##                  ] 11.00%, last=01:23:0.8740 Episode 1100, reward: 12\n",
      "[##                  ] 12.00%, last=01:22:14.062 Episode 1200, reward: 28\n",
      "[##                  ] 13.00%, last=01:21:32.716 Episode 1300, reward: 22\n",
      "[##                  ] 14.00%, last=01:21:3.6466 Episode 1400, reward: 18\n",
      "[###                 ] 15.00%, last=01:20:19.189 Episode 1500, reward: 6\n",
      "[###                 ] 16.00%, last=01:19:32.532 Episode 1600, reward: 24\n",
      "[###                 ] 17.00%, last=01:18:57.415 Episode 1700, reward: 14\n",
      "[###                 ] 18.00%, last=01:18:20.255 Episode 1800, reward: 26\n",
      "[###                 ] 19.00%, last=01:17:39.058 Episode 1900, reward: 16\n",
      "[####                ] 20.00%, last=01:16:59.893 Episode 2000, reward: 30\n",
      "[####                ] 21.00%, last=01:16:21.147 Episode 2100, reward: 16\n",
      "[####                ] 22.00%, last=01:15:38.902 Episode 2200, reward: 20\n",
      "[####                ] 23.00%, last=01:15:0.5295 Episode 2300, reward: 28\n",
      "[####                ] 24.00%, last=01:14:19.022 Episode 2400, reward: 24\n",
      "[#####               ] 25.00%, last=01:13:35.984 Episode 2500, reward: 40\n",
      "[#####               ] 26.00%, last=01:12:54.915 Episode 2600, reward: 28\n",
      "[#####               ] 27.00%, last=01:12:14.943 Episode 2700, reward: 20\n",
      "[#####               ] 28.00%, last=01:11:32.571 Episode 2800, reward: 18\n",
      "[#####               ] 29.00%, last=01:10:45.846 Episode 2900, reward: 34\n",
      "[######              ] 30.00%, last=01:09:58.743 Episode 3000, reward: 38\n",
      "[######              ] 31.00%, last=01:09:9.4402 Episode 3100, reward: 34\n",
      "[######              ] 32.00%, last=01:08:19.120 Episode 3200, reward: 12\n",
      "[######              ] 33.00%, last=01:07:33.544 Episode 3300, reward: 8\n",
      "[######              ] 34.00%, last=01:06:43.165 Episode 3400, reward: 26\n",
      "[#######             ] 35.00%, last=01:05:52.434 Episode 3500, reward: 26\n",
      "[#######             ] 36.00%, last=01:05:1.6810 Episode 3600, reward: 32\n",
      "[#######             ] 37.00%, last=01:04:10.334 Episode 3700, reward: 18\n",
      "[#######             ] 38.00%, last=01:03:20.065 Episode 3800, reward: 36\n",
      "[#######             ] 39.00%, last=01:02:30.501 Episode 3900, reward: 2\n",
      "[########            ] 40.00%, last=01:01:40.179 Episode 4000, reward: 42\n",
      "[########            ] 41.00%, last=01:00:50.999 Episode 4100, reward: 18\n",
      "[########            ] 42.00%, last=01:00:0.2189 Episode 4200, reward: 38\n",
      "[########            ] 43.00%, last=00:59:8.7026 Episode 4300, reward: 16\n",
      "[########            ] 44.00%, last=00:58:17.095 Episode 4400, reward: 28\n",
      "[#########           ] 45.00%, last=00:57:23.653 Episode 4500, reward: 12\n",
      "[#########           ] 46.00%, last=00:56:28.897 Episode 4600, reward: 30\n",
      "[#########           ] 47.00%, last=00:55:32.693 Episode 4700, reward: 28\n",
      "[#########           ] 48.00%, last=00:54:32.710 Episode 4800, reward: 42\n",
      "[#########           ] 49.00%, last=00:53:33.926 Episode 4900, reward: 58\n",
      "[##########          ] 50.00%, last=00:52:34.352 Episode 5000, reward: 42\n",
      "[##########          ] 51.00%, last=00:51:36.155 Episode 5100, reward: 26\n",
      "[##########          ] 52.00%, last=00:50:36.815 Episode 5200, reward: 34\n",
      "[##########          ] 53.00%, last=00:49:36.283 Episode 5300, reward: 34\n",
      "[##########          ] 54.00%, last=00:48:36.585 Episode 5400, reward: 20\n",
      "[###########         ] 55.00%, last=00:47:36.205 Episode 5500, reward: 30\n",
      "[###########         ] 56.00%, last=00:46:36.428 Episode 5600, reward: 46\n",
      "[###########         ] 57.00%, last=00:45:35.873 Episode 5700, reward: 30\n",
      "[###########         ] 58.00%, last=00:44:36.054 Episode 5800, reward: 24\n",
      "[###########         ] 59.00%, last=00:43:36.164 Episode 5900, reward: 32\n",
      "[############        ] 60.00%, last=00:42:37.152 Episode 6000, reward: 42\n",
      "[############        ] 60.81%, last=00:41:48.766"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_rewards = []\n",
    "\n",
    "for i in range(0, len(episode_rewards)):\n",
    "    # print(f\"Episode {i}, mean reward: {np.mean(episode_rewards[0:i])}\")\n",
    "    mean_rewards.append(np.mean(episode_rewards[0:i]))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_rewards)\n",
    "plt.plot(mean_rewards, color=\"red\")\n",
    "plt.show()"
   ],
   "id": "8d05f9111b71c5e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trained_net = DQN(n_inputs, n_actions).to(device)\n",
    "trained_net.load_state_dict(torch.load(\"ddqn.pth\", weights_only=True))\n",
    "trained_net.eval()\n",
    "\n",
    "state, _ = test_env.reset()\n",
    "max_len = len(raw_data_test) - 1\n",
    "\n",
    "confusion_matrix = [0, 0, 0, 0] # TP, FP, TN, FN\n",
    "total_reward = 0\n",
    "\n",
    "start_time = time()\n",
    "for i in range(max_len):\n",
    "    with torch.no_grad():\n",
    "        action = trained_net(torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)).max(1).indices\n",
    "\n",
    "    next_state, reward, terminated, _, info = test_env.step(action.item())\n",
    "\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    total_reward += reward.item()\n",
    "\n",
    "    confusion_index = (0 if \"P\" in info[\"confusion\"] else 1) if \"T\" in info[\"confusion\"] else (2 if \"P\" in info[\"confusion\"] else 3)\n",
    "    confusion_matrix[confusion_index] += 1\n",
    "\n",
    "    state = next_state\n",
    "    end_time = time()\n",
    "    loading_bar(i, max_len, end_time - start_time)\n",
    "\n",
    "print()\n",
    "test_env.close()\n",
    "\n",
    "print(f\"confusion matrix: {confusion_matrix}\")\n",
    "\n",
    "tp = confusion_matrix[0]\n",
    "tn = confusion_matrix[1]\n",
    "fp = confusion_matrix[2]\n",
    "fn = confusion_matrix[3]\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "print(f\"Accuracy : {accuracy:.5}\")\n",
    "precision = tp / (tp + fp)\n",
    "print(f\"Precision: {precision:.5}\")\n",
    "recall = tp / (tp + fn) if tp + fn != 0 else 0.0\n",
    "print(f\" Recall  : {recall:.5}\")\n",
    "f1 = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0.0\n",
    "print(f\"    F1   : {f1:.5}\")\n",
    "fpr = fp / (fp + tn) if fp + tn != 0 else 0.0\n",
    "print(f\"    FPR  : {fpr:.5}\")\n",
    "\n",
    "metrics = [accuracy, precision, recall, f1, fpr]\n",
    "metrics = [n * 100 for n in metrics]\n",
    "metrics_labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"FPR\"]\n",
    "\n",
    "plt.figure(1)\n",
    "plt.bar(metrics_labels, metrics)\n",
    "plt.grid()\n"
   ],
   "id": "5d587054861dc7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f42030b47b63c5df",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
