{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-31T01:36:10.309095Z",
     "start_time": "2024-12-31T01:36:08.090232Z"
    }
   },
   "source": [
    "\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## train and test data",
   "id": "948e9c999dcca3e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T01:36:16.131280Z",
     "start_time": "2024-12-31T01:36:16.069328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TRAIN_DATA_PATH = \"../../DNP3_Intrusion_Detection_Dataset_Final/Training_Testing_Balanced_CSV_Files/CICFlowMeter/CICFlowMeter_Training_Balanced.csv\"\n",
    "TEST_DATA_PATH = \"../../DNP3_Intrusion_Detection_Dataset_Final/Training_Testing_Balanced_CSV_Files/CICFlowMeter/CICFlowMeter_Testing_Balanced.csv\"\n",
    "\n",
    "# CSVファイルの読み込み\n",
    "raw_data_train = pd.read_csv(TRAIN_DATA_PATH).replace([np.inf, -np.inf], np.nan).dropna(how=\"all\").dropna(how=\"all\", axis=1)\n",
    "raw_data_test = pd.read_csv(TEST_DATA_PATH).dropna(how=\"all\").replace([np.inf, -np.inf], np.nan).dropna(how=\"all\", axis=1)\n",
    "\n",
    "raw_data_train[\"Binary Label\"] = raw_data_train[\"Label\"] == \"NORMAL\"\n",
    "raw_data_test[\"Binary Label\"] = raw_data_test[\"Label\"] == \"NORMAL\"\n",
    "\n",
    "train_env = gym.make(\"flowenv/FlowTrain-v0\", data=raw_data_train)\n",
    "# test_env = gym.make(\"flowenv/FlowTest-v0\", data=raw_data_test)"
   ],
   "id": "9ca53face49634af",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agent",
   "id": "8bac94264d2adc17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T13:36:53.596351Z",
     "start_time": "2024-12-30T13:36:53.592080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class A2C(nn.Module):\n",
    "    def __init__(self, env, hidden_size=128, gamma=0.99, random_seed=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if random_seed:\n",
    "            env.seed(random_seed)\n",
    "            torch.manual_seed(random_seed)\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_size = len(env.observation_space.sample().flatten())\n",
    "        self.out_size = env.action_space.n\n",
    "\n",
    "    def forward(self, state):\n",
    "        pass\n",
    "\n",
    "class Actor(A2C):\n",
    "    def __init__(self, env, hidden_size=128, gamma=0.99, random_seed=None):\n",
    "        super().__init__(env, hidden_size, gamma, random_seed)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.in_size, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.out_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state.clone().detach()))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Critic(A2C):\n",
    "    def __init__(self, env, hidden_size=128, gamma=0.99, random_seed=None):\n",
    "        super().__init__(env, hidden_size, gamma, random_seed)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.in_size, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state.clone().detach()))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ],
   "id": "d315f700113abc46",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T13:36:53.841993Z",
     "start_time": "2024-12-30T13:36:53.838046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Transaction = namedtuple('Transaction', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transaction(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "id": "e15708618a1d1211",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T13:36:54.136814Z",
     "start_time": "2024-12-30T13:36:54.134074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimizer_step():\n",
    "    pass\n",
    "\n",
    "def select_action(arg_state):\n",
    "    state_tensor = torch.tensor(arg_state, dtype=torch.float64)\n",
    "    if torch.isnan(state_tensor).any():\n",
    "        state_tensor = torch.nan_to_num(state_tensor, nan=0.0)\n",
    "\n",
    "    action_logits = actor(state_tensor)\n",
    "    action = Categorical(logits=action_logits).sample()\n",
    "\n",
    "    return action"
   ],
   "id": "5f0c53ffbcf84be6",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loss\n",
    "\n",
    "$$\n",
    "\\text{Actor Loss} = - \\mathbb{E}[\\log \\pi(a_t|s_t) \\cdot A_t]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Critic Loss} = \\frac{1}{2} \\mathbb{E}[(V(s_t) - R_t)^2]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Total Loss} = \\text{Actor Loss} + \\lambda \\cdot \\text{Critic Loss}\n",
    "$$"
   ],
   "id": "5a01e764264cc8d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T13:42:15.706731Z",
     "start_time": "2024-12-30T13:41:52.325616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# const values\n",
    "BATCH_SIZE = 64\n",
    "LAMBDA = 0.5\n",
    "\n",
    "# variables\n",
    "actor = Actor(train_env)\n",
    "critic = Critic(train_env)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e3)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e3)\n",
    "\n",
    "action_lp_vals = []\n",
    "critic_vals = []\n",
    "rewards = []\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "def optimizer_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    critic_optimizer.zero_grad()\n",
    "    actor_optimizer.zero_grad()\n",
    "\n",
    "    transactions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transaction(*zip(*transactions))\n",
    "\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.stack(batch.action).unsqueeze(1)\n",
    "    reward_batch = torch.tensor(batch.reward, dtype=torch.float32)\n",
    "    next_state_batch = torch.stack([torch.tensor(ns, dtype=torch.float32) for ns in batch.next_state])\n",
    "\n",
    "    values = critic(state_batch).squeeze()\n",
    "    next_values = critic(next_state_batch).squeeze()\n",
    "    targets = reward_batch + critic.gamma * next_values\n",
    "    advantages = targets - values\n",
    "\n",
    "    action_probs = actor(state_batch)\n",
    "    action_log_props = torch.log(action_probs.gather(1, action_batch))\n",
    "    actor_loss = -torch.mean(action_log_props * advantages.detach())\n",
    "\n",
    "    critic_loss = nn.MSELoss()(values, targets.detach())\n",
    "\n",
    "    total_loss = actor_loss + LAMBDA * critic_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    utils.clip_grad_norm_(actor.parameters(), 1.0)\n",
    "    utils.clip_grad_norm_(critic.parameters(), 1.0)\n",
    "\n",
    "    actor_optimizer.step()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "for i_episode in range(1000):\n",
    "    state, _ = train_env.reset()\n",
    "    done = False\n",
    "    sum_rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        if torch.isnan(state).any():\n",
    "            state = torch.nan_to_num(state, nan=1e-6)\n",
    "            print(\"nan detected\")\n",
    "        action_logits = actor(state)\n",
    "        if torch.isnan(action_logits).any():\n",
    "            print(state)\n",
    "            print(action_logits)\n",
    "            # action_logits = torch.nan_to_num(action_logits, nan=0.0)\n",
    "        action = Categorical(logits=action_logits).sample()\n",
    "\n",
    "        action_log_prob = action_logits[action]\n",
    "        pred = torch.squeeze(critic(state).view(-1))\n",
    "\n",
    "        action_lp_vals.append(action_log_prob)\n",
    "        critic_vals.append(pred)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = train_env.step(action)\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        done = terminated or truncated\n",
    "        sum_rewards += reward\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    rewards.append(sum_rewards)\n",
    "    print(f\"Episode {i_episode:4} finished with reward {sum_rewards:+2}, length of memory {len(memory)}\")\n",
    "    optimizer_model()\n"
   ],
   "id": "3d4dce796b6f1d00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 finished with reward +78, length of memory 100\n",
      "Episode    1 finished with reward +12, length of memory 200\n",
      "Episode    2 finished with reward +82, length of memory 300\n",
      "Episode    3 finished with reward +76, length of memory 400\n",
      "Episode    4 finished with reward +78, length of memory 500\n",
      "Episode    5 finished with reward +80, length of memory 600\n",
      "Episode    6 finished with reward -6, length of memory 700\n",
      "Episode    7 finished with reward -12, length of memory 800\n",
      "Episode    8 finished with reward -2, length of memory 900\n",
      "Episode    9 finished with reward -14, length of memory 1000\n",
      "Episode   10 finished with reward -8, length of memory 1100\n",
      "Episode   11 finished with reward -4, length of memory 1200\n",
      "Episode   12 finished with reward -8, length of memory 1300\n",
      "Episode   13 finished with reward +0, length of memory 1400\n",
      "Episode   14 finished with reward +4, length of memory 1500\n",
      "Episode   15 finished with reward -8, length of memory 1600\n",
      "Episode   16 finished with reward -10, length of memory 1700\n",
      "Episode   17 finished with reward -36, length of memory 1800\n",
      "Episode   18 finished with reward -20, length of memory 1900\n",
      "Episode   19 finished with reward -36, length of memory 2000\n",
      "Episode   20 finished with reward -14, length of memory 2026\n",
      "Episode   21 finished with reward -82, length of memory 2126\n",
      "Episode   22 finished with reward -94, length of memory 2226\n",
      "Episode   23 finished with reward -86, length of memory 2326\n",
      "Episode   24 finished with reward -76, length of memory 2426\n",
      "Episode   25 finished with reward -80, length of memory 2526\n",
      "Episode   26 finished with reward -82, length of memory 2626\n",
      "Episode   27 finished with reward -84, length of memory 2726\n",
      "Episode   28 finished with reward -96, length of memory 2826\n",
      "Episode   29 finished with reward -80, length of memory 2926\n",
      "Episode   30 finished with reward -94, length of memory 3026\n",
      "Episode   31 finished with reward -88, length of memory 3126\n",
      "Episode   32 finished with reward -82, length of memory 3226\n",
      "Episode   33 finished with reward -82, length of memory 3326\n",
      "Episode   34 finished with reward -80, length of memory 3426\n",
      "Episode   35 finished with reward -92, length of memory 3526\n",
      "Episode   36 finished with reward -72, length of memory 3626\n",
      "Episode   37 finished with reward -76, length of memory 3726\n",
      "Episode   38 finished with reward -86, length of memory 3826\n",
      "Episode   39 finished with reward -90, length of memory 3926\n",
      "Episode   40 finished with reward -84, length of memory 4026\n",
      "Episode   41 finished with reward -80, length of memory 4126\n",
      "Episode   42 finished with reward -78, length of memory 4226\n",
      "Episode   43 finished with reward -80, length of memory 4326\n",
      "Episode   44 finished with reward -84, length of memory 4426\n",
      "Episode   45 finished with reward -78, length of memory 4526\n",
      "Episode   46 finished with reward -84, length of memory 4626\n",
      "Episode   47 finished with reward -72, length of memory 4726\n",
      "Episode   48 finished with reward -72, length of memory 4826\n",
      "Episode   49 finished with reward -74, length of memory 4926\n",
      "nan detected\n",
      "Episode   50 finished with reward -84, length of memory 5026\n",
      "Episode   51 finished with reward -90, length of memory 5126\n",
      "Episode   52 finished with reward -80, length of memory 5226\n",
      "Episode   53 finished with reward -84, length of memory 5326\n",
      "Episode   54 finished with reward -70, length of memory 5426\n",
      "Episode   55 finished with reward -68, length of memory 5526\n",
      "Episode   56 finished with reward -72, length of memory 5626\n",
      "Episode   57 finished with reward -74, length of memory 5726\n",
      "Episode   58 finished with reward -70, length of memory 5826\n",
      "Episode   59 finished with reward -60, length of memory 5926\n",
      "Episode   60 finished with reward -70, length of memory 6026\n",
      "Episode   61 finished with reward -74, length of memory 6126\n",
      "Episode   62 finished with reward -82, length of memory 6226\n",
      "Episode   63 finished with reward -82, length of memory 6326\n",
      "Episode   64 finished with reward -84, length of memory 6426\n",
      "Episode   65 finished with reward -78, length of memory 6526\n",
      "Episode   66 finished with reward -76, length of memory 6626\n",
      "Episode   67 finished with reward -74, length of memory 6726\n",
      "Episode   68 finished with reward -88, length of memory 6826\n",
      "Episode   69 finished with reward -78, length of memory 6926\n",
      "Episode   70 finished with reward -92, length of memory 7026\n",
      "Episode   71 finished with reward -88, length of memory 7126\n",
      "Episode   72 finished with reward -26, length of memory 7152\n",
      "Episode   73 finished with reward -82, length of memory 7252\n",
      "Episode   74 finished with reward -94, length of memory 7352\n",
      "Episode   75 finished with reward -86, length of memory 7452\n",
      "Episode   76 finished with reward -78, length of memory 7552\n",
      "Episode   77 finished with reward -82, length of memory 7652\n",
      "Episode   78 finished with reward -86, length of memory 7752\n",
      "Episode   79 finished with reward -84, length of memory 7852\n",
      "Episode   80 finished with reward -96, length of memory 7952\n",
      "Episode   81 finished with reward -80, length of memory 8052\n",
      "Episode   82 finished with reward -94, length of memory 8152\n",
      "Episode   83 finished with reward -88, length of memory 8252\n",
      "Episode   84 finished with reward -82, length of memory 8352\n",
      "Episode   85 finished with reward -82, length of memory 8452\n",
      "Episode   86 finished with reward -80, length of memory 8552\n",
      "Episode   87 finished with reward -92, length of memory 8652\n",
      "Episode   88 finished with reward -72, length of memory 8752\n",
      "Episode   89 finished with reward -76, length of memory 8852\n",
      "Episode   90 finished with reward -86, length of memory 8952\n",
      "Episode   91 finished with reward -90, length of memory 9052\n",
      "Episode   92 finished with reward -84, length of memory 9152\n",
      "Episode   93 finished with reward -80, length of memory 9252\n",
      "Episode   94 finished with reward -78, length of memory 9352\n",
      "Episode   95 finished with reward -80, length of memory 9452\n",
      "Episode   96 finished with reward -84, length of memory 9552\n",
      "Episode   97 finished with reward -78, length of memory 9652\n",
      "Episode   98 finished with reward -84, length of memory 9752\n",
      "Episode   99 finished with reward -72, length of memory 9852\n",
      "Episode  100 finished with reward -72, length of memory 9952\n",
      "Episode  101 finished with reward -74, length of memory 10000\n",
      "nan detected\n",
      "Episode  102 finished with reward -84, length of memory 10000\n",
      "Episode  103 finished with reward -90, length of memory 10000\n",
      "Episode  104 finished with reward -80, length of memory 10000\n",
      "Episode  105 finished with reward -84, length of memory 10000\n",
      "Episode  106 finished with reward -82, length of memory 10000\n",
      "Episode  107 finished with reward -76, length of memory 10000\n",
      "Episode  108 finished with reward -78, length of memory 10000\n",
      "Episode  109 finished with reward -80, length of memory 10000\n",
      "Episode  110 finished with reward -76, length of memory 10000\n",
      "Episode  111 finished with reward -72, length of memory 10000\n",
      "Episode  112 finished with reward -74, length of memory 10000\n",
      "Episode  113 finished with reward -74, length of memory 10000\n",
      "Episode  114 finished with reward -82, length of memory 10000\n",
      "Episode  115 finished with reward -82, length of memory 10000\n",
      "Episode  116 finished with reward -84, length of memory 10000\n",
      "Episode  117 finished with reward -78, length of memory 10000\n",
      "Episode  118 finished with reward -76, length of memory 10000\n",
      "Episode  119 finished with reward -74, length of memory 10000\n",
      "Episode  120 finished with reward -88, length of memory 10000\n",
      "Episode  121 finished with reward -78, length of memory 10000\n",
      "Episode  122 finished with reward -92, length of memory 10000\n",
      "Episode  123 finished with reward -88, length of memory 10000\n",
      "Episode  124 finished with reward -26, length of memory 10000\n",
      "tensor([ 2.0003e+04,  6.0000e+00,  1.1936e+08,  6.1000e+01,  6.5000e+01,\n",
      "         1.1070e+03,  2.0030e+03,  2.7000e+01,  1.8000e+01,  1.8148e+01,\n",
      "         1.1523e+00,  2.9200e+02,  1.7000e+01,  3.0815e+01,  5.5349e+01,\n",
      "         2.6055e+01,  1.0556e+00,  9.5489e+05,  9.9334e+05,  2.0691e+06,\n",
      "         3.4900e+02,  1.1838e+08,  1.9730e+06,  1.8383e+05,  2.0699e+06,\n",
      "         9.0004e+05,  1.1936e+08,  1.8650e+06,  4.6976e+05,  2.0697e+06,\n",
      "         3.4900e+02,  0.0000e+00,  1.9520e+03,  2.0800e+03,  5.1106e-01,\n",
      "         5.4457e-01,  1.7000e+01,  2.9200e+02,  2.4701e+01,  3.9960e+01,\n",
      "         1.5968e+03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  1.0000e+00,  2.4897e+01,  1.8148e+01,\n",
      "         3.0815e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1000e+01,\n",
      "         1.1070e+03,  6.5000e+01,  2.0030e+03, -1.0000e+00,  2.2700e+02,\n",
      "         6.1000e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])\n",
      "tensor([nan, nan], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter logits (Tensor of shape (2,)) of distribution Categorical(logits: torch.Size([2])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([nan, nan], grad_fn=<SubBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[73], line 66\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28mprint\u001B[39m(action_logits)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# action_logits = torch.nan_to_num(action_logits, nan=0.0)\u001B[39;00m\n\u001B[1;32m---> 66\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[43mCategorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction_logits\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msample()\n\u001B[0;32m     68\u001B[0m action_log_prob \u001B[38;5;241m=\u001B[39m action_logits[action]\n\u001B[0;32m     69\u001B[0m pred \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqueeze(critic(state)\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\PycharmProjects\\machine-learning\\.venv\\Lib\\site-packages\\torch\\distributions\\categorical.py:72\u001B[0m, in \u001B[0;36mCategorical.__init__\u001B[1;34m(self, probs, logits, validate_args)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_events \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     69\u001B[0m batch_shape \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39mndimension() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mSize()\n\u001B[0;32m     71\u001B[0m )\n\u001B[1;32m---> 72\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\machine-learning\\.venv\\Lib\\site-packages\\torch\\distributions\\distribution.py:71\u001B[0m, in \u001B[0;36mDistribution.__init__\u001B[1;34m(self, batch_shape, event_shape, validate_args)\u001B[0m\n\u001B[0;32m     69\u001B[0m         valid \u001B[38;5;241m=\u001B[39m constraint\u001B[38;5;241m.\u001B[39mcheck(value)\n\u001B[0;32m     70\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid\u001B[38;5;241m.\u001B[39mall():\n\u001B[1;32m---> 71\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     72\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected parameter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     73\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(value)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtuple\u001B[39m(value\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     74\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof distribution \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     75\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto satisfy the constraint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(constraint)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     76\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut found invalid values:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     77\u001B[0m             )\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mValueError\u001B[0m: Expected parameter logits (Tensor of shape (2,)) of distribution Categorical(logits: torch.Size([2])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([nan, nan], grad_fn=<SubBackward0>)"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T13:41:21.023915Z",
     "start_time": "2024-12-30T13:41:21.021761Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "15fc2e52d28eac6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "78af08b4443384ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
