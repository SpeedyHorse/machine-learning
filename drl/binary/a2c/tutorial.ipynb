{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "lr = 0.002\n",
    "gamma = 0.99\n",
    "episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared_layer(x)\n",
    "        policy = self.actor(shared)\n",
    "        value = self.critic(shared)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "model = ActorCritic(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def a2c():\n",
    "    all_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "\n",
    "        while True:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            policy, value = model(state_tensor)\n",
    "            action_dist = torch.distributions.Categorical(policy)\n",
    "            action = action_dist.sample()\n",
    "\n",
    "            log_probs.append(action_dist.log_prob(action))\n",
    "            values.append(value)\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(np.mean(rewards))\n",
    "        cumulative_reward = 0\n",
    "        discounted_rewards = []\n",
    "        for r in reversed(rewards):\n",
    "            cumulative_reward = r + gamma * cumulative_reward\n",
    "            discounted_rewards.insert(0, cumulative_reward)\n",
    "        \n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "        values = torch.cat(values).squeeze()\n",
    "        advantages = discounted_rewards - values.detach()\n",
    "\n",
    "        actor_loss = - (torch.stack(log_probs) * advantages).sum()\n",
    "        critic_loss = nn.functional.mse_loss(values, discounted_rewards)\n",
    "        loss = actor_loss + critic_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {sum(rewards)}\")\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(all_rewards)\n",
    "    means = [all_rewards[0]]\n",
    "    for i in range(1, len(all_rewards)):\n",
    "        means.append(np.mean(all_rewards[0:i]))\n",
    "    plt.plot(means, color=\"red\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 33.0\n",
      "Episode: 50, Total Reward: 20.0\n",
      "Episode: 100, Total Reward: 89.0\n",
      "Episode: 150, Total Reward: 53.0\n",
      "Episode: 200, Total Reward: 111.0\n",
      "Episode: 250, Total Reward: 18.0\n",
      "Episode: 300, Total Reward: 234.0\n",
      "Episode: 350, Total Reward: 168.0\n",
      "Episode: 400, Total Reward: 165.0\n",
      "Episode: 450, Total Reward: 681.0\n",
      "Episode: 500, Total Reward: 29.0\n",
      "Episode: 550, Total Reward: 54.0\n",
      "Episode: 600, Total Reward: 104.0\n",
      "Episode: 650, Total Reward: 104.0\n",
      "Episode: 700, Total Reward: 133.0\n",
      "Episode: 750, Total Reward: 294.0\n"
     ]
    }
   ],
   "source": [
    "a2c()\n",
    "\n",
    "state = env.reset()[0]\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    policy, _ = model(state_tensor)\n",
    "    action = policy.argmax().item()\n",
    "    state, _, done, _, _ = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
