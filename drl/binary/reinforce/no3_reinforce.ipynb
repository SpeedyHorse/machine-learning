{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9829546b457e4ca2",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import random\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "from time import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a7a0da379a7907",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device_name = \"cpu\"\n",
    "\n",
    "if True:\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = \"cuda\"\n",
    "    elif torch.mps.is_available():\n",
    "        device_name = \"mps\"\n",
    "    # elif torch.hip.is_available():\n",
    "    #     device_name = \"hip\"\n",
    "    elif torch.mtia.is_available():\n",
    "        device_name = \"mtia\"\n",
    "    elif torch.xpu.is_available():\n",
    "        device_name = \"xpu\"\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(f\"device: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ad38480eece59",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c20eb4be516024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/toshi_pro/Documents/github-sub/machine-learning\")\n",
    "# sys.path.append(\"/Users/toshi/Documents/school/machine-learning\")\n",
    "# sys.path.append(r\"C:\\Users\\takat\\PycharmProjects\\machine-learning\")\n",
    "import flowdata\n",
    "import flowenv\n",
    "\n",
    "raw_data_train, raw_data_test = flowdata.flow_data.using_data()\n",
    "raw_data_train.dropna(how=\"any\")\n",
    "raw_data_test.dropna(how=\"any\")\n",
    "# print(raw_data_train)\n",
    "# train_env = gym.make(\"flowenv/FlowTrain-v0\", data=raw_data_train)\n",
    "train_env = gym.make(\"flowenv/Flow-v1\", data=raw_data_train)\n",
    "# test_env = gym.make(\"flowenv/FlowTest-v0\", data=raw_data_test)\n",
    "test_env = gym.make(\"flowenv/Flow-v1\", data=raw_data_test)\n",
    "\n",
    "# pd.set_option('display.max_columns', 1000)\n",
    "# print(raw_data_train[raw_data_train[\"Dst Port\"] == 39964])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cd4e937dd0e7a",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2c5d2dc4692746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transaction = namedtuple('Transaction', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transaction(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "Trajectory = namedtuple('Trajectory', (\"rewards\", \"log_probs\"))\n",
    "\n",
    "class EpisodeMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Trajectory(*args))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    # last batch_size memory output\n",
    "    def sample(self, batch_size):\n",
    "        return list(self.memory)[-batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366eff4c463bc1",
   "metadata": {},
   "source": [
    "## Plot rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986abee18b347dab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_rewards(rewards: list, show_result=False):\n",
    "    plt.figure(1)\n",
    "    # durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title(\"Result\")\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(\"Training...\")\n",
    "    means = [rewards[0]]\n",
    "    for i in range(1, len(rewards)):\n",
    "        means.append(np.mean(rewards[0:i]))\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    # plt.plot(rewards)\n",
    "    plt.plot(means, color=\"red\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8d088cc400a79",
   "metadata": {},
   "source": [
    "### Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f781b493f2dee8a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict: dict, show_result=False):\n",
    "    display.clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "    ac = fig.add_subplot(3, 2, 1)\n",
    "    ac.plot(metrics_dict[\"accuracy\"], label=\"accuracy\")\n",
    "    ac.grid()\n",
    "    ac.set_title(\"Accuracy\")\n",
    "\n",
    "    pr = fig.add_subplot(3, 2, 2)\n",
    "    pr.plot(metrics_dict[\"precision\"], label=\"precision\", color=\"green\")\n",
    "    pr.grid()\n",
    "    pr.set_title(\"Precision\")\n",
    "\n",
    "    re = fig.add_subplot(3, 2, 3)\n",
    "    re.plot(metrics_dict[\"recall\"], label=\"recall\", color=\"red\")\n",
    "    re.grid()\n",
    "    re.set_title(\"Recall\")\n",
    "\n",
    "    f1 = fig.add_subplot(3, 2, 4)\n",
    "    f1.plot(metrics_dict[\"f1\"], label=\"f1\", color=\"black\")\n",
    "    f1.grid()\n",
    "    f1.set_title(\"F1\")\n",
    "\n",
    "    fpr = fig.add_subplot(3, 2, 5)\n",
    "    fpr.plot(metrics_dict[\"fpr\"], label=\"fpr\", color=\"purple\")\n",
    "    fpr.grid()\n",
    "    fpr.set_title(\"FPR\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "\n",
    "def calculate_metrics(tp, tn, fp, fn):\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else -1\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else -1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if fp + tn != 0 else 0.0\n",
    "\n",
    "    if precision < 0:\n",
    "        precision = 0.0\n",
    "    if recall < 0:\n",
    "        recall = 0.0\n",
    "    return accuracy, precision, recall, f1, fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a75ff736ef3f0",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a004798cb2fc841",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.common_fc = nn.Sequential(\n",
    "            nn.Linear(n_inputs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.probs = nn.Sequential(\n",
    "            nn.Linear(128, n_outputs),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.fc_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.common_fc(x)\n",
    "        probs = self.probs(x)\n",
    "        value = self.fc_value(x)\n",
    "        return probs, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ee49e58d3557a",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa23955fd1c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52e62d70492c4",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45fb7bba5e7d4c19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "\n",
    "n_inputs = train_env.observation_space.shape[0]\n",
    "n_outputs = train_env.action_space.n\n",
    "\n",
    "policy_net = PolicyNetwork(n_inputs, n_outputs).to(device)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "\n",
    "steps_done = 0\n",
    "memory = ReplayMemory(1000000)\n",
    "episode_memory = EpisodeMemory(100000)\n",
    "episode_rewards = []\n",
    "returns = []\n",
    "episode_accuracy = []\n",
    "episode_metrics = {\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": [],\n",
    "    \"fpr\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ad37010e273fd",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f71fba0aad48cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state: torch.Tensor):\n",
    "    prob_distri, _ = policy_net(state) # return probability of actions\n",
    "    try:\n",
    "        action = torch.multinomial(prob_distri, 1)\n",
    "    except:\n",
    "        raise Exception(state, prob_distri)\n",
    "    return prob_distri, action # return index of action\n",
    "\n",
    "def calculate_returns(rewards):\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    G = 0\n",
    "    try:\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            G = rewards[i] + GAMMA * G\n",
    "            returns[i] = G\n",
    "    except:\n",
    "        returns[0] = rewards[0]\n",
    "    return returns.clone().detach().requires_grad_(True)\n",
    "\n",
    "def optimize_model():\n",
    "    # print(log_probs)\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    trajectory = episode_memory.sample(BATCH_SIZE)\n",
    "    batch = Trajectory(*zip(*trajectory))\n",
    "\n",
    "    rewards = torch.cat(batch.rewards).squeeze()\n",
    "    log_probs = torch.cat(batch.log_probs).squeeze()\n",
    "\n",
    "    returns = calculate_returns(rewards)\n",
    "    baseline = returns.mean()\n",
    "    advantage = returns - baseline\n",
    "\n",
    "    loss = -(log_probs * advantage).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b2384c576de98",
   "metadata": {},
   "source": [
    "REINFORCE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcffe90ed4225c6",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4d16a6f258368",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    MODEL_PATH = \"no3_reinforce.pth\"\n",
    "\n",
    "    # load the model\n",
    "    trained_network = PolicyNetwork(n_inputs, n_outputs).to(device)\n",
    "    trained_network.load_state_dict(torch.load(MODEL_PATH, map_location=device, weights_only=True))\n",
    "    trained_network.eval()\n",
    "\n",
    "    # test the model\n",
    "\n",
    "    confusion_array = np.zeros((2, 2), dtype=np.int32)\n",
    "    metrics_dictionary = {\n",
    "        \"accuracy\": [],\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1\": [],\n",
    "        \"fpr\": []\n",
    "    }\n",
    "\n",
    "    for i_loop in range(100):\n",
    "        test_raw_state, _ = test_env.reset()\n",
    "        test_state = torch.tensor(test_raw_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        for t in count():\n",
    "            with torch.no_grad():\n",
    "                prob_distribution, _ = trained_network(test_state)\n",
    "                test_action = torch.multinomial(prob_distribution, 1)\n",
    "\n",
    "            test_raw_next_state, test_reward, test_terminated, test_truncated, test_info = test_env.step(test_action.item())\n",
    "\n",
    "            # calculate confusion matrix\n",
    "            raw = 0 if test_reward == 1 else 1\n",
    "\n",
    "            # test_info = (row, column) means confusion matrix index\n",
    "            index = test_info[\"confusion_position\"]\n",
    "            confusion_array[index[0], index[1]] += 1\n",
    "\n",
    "            if test_terminated:\n",
    "                break\n",
    "\n",
    "            # make next state tensor and update state\n",
    "            test_state = torch.tensor(test_raw_next_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # calculate metrics\n",
    "        tp = confusion_array[1, 1]\n",
    "        tn = confusion_array[0, 0]\n",
    "        fp = confusion_array[1, 0]\n",
    "        fn = confusion_array[0, 1]\n",
    "\n",
    "        accuracy, precision, recall, f1, fpr = calculate_metrics(tp, tn, fp, fn)\n",
    "        metrics_dictionary[\"accuracy\"].append(accuracy)\n",
    "        metrics_dictionary[\"precision\"].append(precision)\n",
    "        metrics_dictionary[\"recall\"].append(recall)\n",
    "        metrics_dictionary[\"f1\"].append(f1)\n",
    "        metrics_dictionary[\"fpr\"].append(fpr)\n",
    "        # print(tp, tn, fp, tn)\n",
    "\n",
    "    return [np.mean(metrics_dictionary[\"accuracy\"]), np.mean(metrics_dictionary[\"precision\"]), np.mean(metrics_dictionary[\"recall\"]), np.mean(metrics_dictionary[\"f1\"]), np.mean(metrics_dictionary[\"fpr\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bad6848-1ef4-497a-9f97-3e65c1fab15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2225ab0d482517",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2048"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(raw_next_state, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(next_state)\n\u001b[1;32m     30\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    random.seed(i_episode)\n",
    "    confusion_matrix = np.zeros((2,2), dtype=int)\n",
    "    sum_reward = 0\n",
    "    # print(i_episode)\n",
    "\n",
    "    initial_state, info = train_env.reset()\n",
    "    state = torch.tensor(initial_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "\n",
    "    for t in count():\n",
    "        # print(f\"State before step - Max: {state.max()}, Min: {state.min()}, Mean: {state.mean()}\")\n",
    "        # select action\n",
    "        prob_distri, action = select_action(state)\n",
    "\n",
    "        # calculate next state\n",
    "        raw_next_state, reward, terminated, truncated, info = train_env.step(action.item())\n",
    "        row_column_index = info[\"confusion_position\"]\n",
    "        confusion_matrix[row_column_index[0], row_column_index[1]] += 1\n",
    "        # to tensor\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(raw_next_state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "            if torch.isnan(next_state).any():\n",
    "                raise Exception(next_state)\n",
    "        reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        sum_reward += reward.item() if reward.item() == 1 else 0\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(torch.log(prob_distri.gather(1, action)))\n",
    "\n",
    "        # move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # optimize the model\n",
    "        # print(len(rewards), len(log_probs))\n",
    "        if terminated:\n",
    "            episode_rewards.append(sum_reward / (t + 1))\n",
    "            break\n",
    "\n",
    "    # do after the episode\n",
    "    rewards = torch.cat(rewards)\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    episode_memory.push(rewards, log_probs)\n",
    "    if i_episode > 0 and i_episode % BATCH_SIZE == 0:\n",
    "        print(f\"\\r{i_episode:5}\", end=\"\")\n",
    "        optimize_model()\n",
    "\n",
    "    ## graph\n",
    "    base = confusion_matrix[1, 1] + confusion_matrix[1, 0]\n",
    "    episode_accuracy.append(\n",
    "        confusion_matrix[0,0] / base if base != 0 else 0.0\n",
    "    )\n",
    "    if i_episode > 0 and i_episode % 100 == 0:\n",
    "        plot_rewards(episode_accuracy)\n",
    "    ##\n",
    "else:\n",
    "    torch.save(policy_net.state_dict(), \"no3_reinforce.pth\")  # save the model\n",
    "\"\"\"\n",
    "    if i_episode > 0 and i_episode % 100 == 0:\n",
    "        torch.save(policy_net.state_dict(), \"no3_reinforce.pth\")  # save the model\n",
    "\n",
    "        print(f\"Episode {i_episode}: {sum_reward}\")\n",
    "        ac, pr, re, f1, fp = test()\n",
    "        episode_metrics[\"accuracy\"].append(ac)\n",
    "        episode_metrics[\"precision\"].append(pr)\n",
    "        episode_metrics[\"recall\"].append(re)\n",
    "        episode_metrics[\"f1\"].append(f1)\n",
    "        episode_metrics[\"fpr\"].append(fp)\n",
    "        plot_metrics(episode_metrics)\n",
    "\"\"\"\n",
    "\n",
    "# complete the episode\n",
    "plot_metrics(episode_metrics, show_result=True)\n",
    "plot_rewards(episode_rewards, show_result=True)\n",
    "\n",
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cd6864d-8fb3-47be-952d-8a2cac4a79c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2073: 61.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PolicyNetwork:\n\tMissing key(s) in state_dict: \"fc_value.weight\", \"fc_value.bias\". \n\tsize mismatch for common_fc.0.weight: copying a param with shape torch.Size([128, 70]) from checkpoint, the shape in current model is torch.Size([128, 652]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_episode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msum_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ac, pr, re, f1, fp \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m episode_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(ac)\n\u001b[1;32m      4\u001b[0m episode_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(pr)\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trained_network \u001b[38;5;241m=\u001b[39m PolicyNetwork(n_inputs, n_outputs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrained_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m trained_network\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# test the model\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github-sub/machine-learning/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PolicyNetwork:\n\tMissing key(s) in state_dict: \"fc_value.weight\", \"fc_value.bias\". \n\tsize mismatch for common_fc.0.weight: copying a param with shape torch.Size([128, 70]) from checkpoint, the shape in current model is torch.Size([128, 652])."
     ]
    }
   ],
   "source": [
    "print(f\"Episode {i_episode}: {sum_reward}\")\n",
    "ac, pr, re, f1, fp = test()\n",
    "episode_metrics[\"accuracy\"].append(ac)\n",
    "episode_metrics[\"precision\"].append(pr)\n",
    "episode_metrics[\"recall\"].append(re)\n",
    "episode_metrics[\"f1\"].append(f1)\n",
    "episode_metrics[\"fpr\"].append(fp)\n",
    "plot_metrics(episode_metrics, show_result=True)\n",
    "print(ac, pr, re, f1, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9740beca7384b44",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcf181-2fcc-4004-9f2c-5f41762b7f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
