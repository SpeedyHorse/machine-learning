{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306d890a5a1da2cc",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n",
    "from time import time\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\takat\\PycharmProjects\\machine-learning\")\n",
    "import flowdata\n",
    "import flowenv\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f342159d2d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = \"cpu\"\n",
    "\n",
    "if True:\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = \"cuda\"\n",
    "    elif torch.mps.is_available():\n",
    "        device_name = \"mps\"\n",
    "    # elif torch.hip.is_available():\n",
    "    #     device_name = \"hip\"\n",
    "    elif torch.mtia.is_available():\n",
    "        device_name = \"mtia\"\n",
    "    elif torch.xpu.is_available():\n",
    "        device_name = \"xpu\"\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(f\"device: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "558eaae66aad8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.2\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e24ff2f0800f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train, raw_data_test = flowdata.flow_data.using_data()\n",
    "\n",
    "train_env = gym.make(\"flowenv/FlowTrain-v0\", data=raw_data_train)\n",
    "test_env = gym.make(\"flowenv/FlowTest-v0\", data=raw_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b0e5fa46466d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 128)\n",
    "        self.fc2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, n_outputs)\n",
    "        self.fc4 = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca015026a90104b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transaction = namedtuple('Transaction', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transaction(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "815c89de3959cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global policy_net\n",
    "    probs = policy_net(state)\n",
    "    return Categorical(probs).sample()\n",
    "\n",
    "def optimize_model():\n",
    "    global log_probs, policy_net, memory\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transaction(*zip(*transitions))\n",
    "\n",
    "    states = torch.cat(batch.state)\n",
    "    actions = torch.cat(batch.action)\n",
    "    rewards = torch.cat(batch.reward)\n",
    "\n",
    "    G = 0\n",
    "    returns = []\n",
    "    \n",
    "    for r in torch.flip(rewards, dims=[0]):\n",
    "        G = r + GAMMA * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    returns = torch.tensor(returns, device=device, dtype=torch.float32)\n",
    "\n",
    "    old_probs = policy_net(states).gather(1, actions).detach()\n",
    "\n",
    "    for _ in range(5):\n",
    "        new_props = policy_net(states).gather(1, actions)\n",
    "        ratio = new_props / old_probs\n",
    "\n",
    "        advantages = returns - returns.mean()\n",
    "\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - EPSILON, 1 + EPSILON) * advantages\n",
    "        loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def get_h_m_s(seconds: float):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds - hours * 3600) // 60)\n",
    "    seconds = seconds - hours * 3600 - minutes * 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "def loading_bar(episode, total_episodes, interval):\n",
    "    pro_size_float = (episode + 1) / total_episodes * 20\n",
    "    show = pro_size_float * 5\n",
    "    pro_size = int(pro_size_float)\n",
    "\n",
    "    # episode...interval -> total_episodes...interval * total_episodes / episode\n",
    "    last_time = interval * (total_episodes - episode) / (episode + 1)\n",
    "    hours, minutes, seconds = get_h_m_s(last_time)\n",
    "    print(f\"\\r[{'#' * pro_size}{' ' * (20 - pro_size)}] {show:3.02f}%, last={hours:02d}:{minutes:02d}:{seconds:03.3f}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a63099640e89d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_metrics = {\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": [],\n",
    "    \"fpr\": []\n",
    "}\n",
    "\n",
    "def plot_metrics(show_result=False):\n",
    "    fig = plt.figure(figsize=(16, 20))\n",
    "\n",
    "    ac = fig.add_subplot(5, 1, 1)\n",
    "    ac.plot(episode_metrics[\"accuracy\"], label=\"accuracy\")\n",
    "    ac.grid()\n",
    "    ac.set_title(\"Accuracy\")\n",
    "\n",
    "    pr = fig.add_subplot(5, 1, 2)\n",
    "    pr.plot(episode_metrics[\"precision\"], label=\"precision\", color=\"green\")\n",
    "    pr.grid()\n",
    "    pr.set_title(\"Precision\")\n",
    "\n",
    "    re = fig.add_subplot(5, 1, 3)\n",
    "    re.plot(episode_metrics[\"recall\"], label=\"recall\", color=\"red\")\n",
    "    re.grid()\n",
    "    re.set_title(\"Recall\")\n",
    "\n",
    "    f1 = fig.add_subplot(5, 1, 4)\n",
    "    f1.plot(episode_metrics[\"f1\"], label=\"f1\", color=\"black\")\n",
    "    f1.grid()\n",
    "    f1.set_title(\"F1\")\n",
    "\n",
    "    fpr = fig.add_subplot(5, 1, 5)\n",
    "    fpr.plot(episode_metrics[\"fpr\"], label=\"fpr\", color=\"purple\")\n",
    "    fpr.grid()\n",
    "    fpr.set_title(\"FPR\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "\n",
    "def calcurate_metrics(tp, tn, fp, fn):\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else -1\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else -1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall < 0 else None\n",
    "    fpr = fp / (fp + tn) if fp + tn != 0 else None\n",
    "\n",
    "    if precision < 0:\n",
    "        precision = None\n",
    "    if recall < 0:\n",
    "        recall = None\n",
    "    return accuracy, precision, recall, f1, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96aee87f30982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "[                    ] 0.01%, last=00:23:55.931<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "[                    ] 0.02%, last=00:13:35.451<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "[                    ] 0.03%, last=00:10:31.938<class 'torch.Tensor'>\n",
      "[                    ] 0.04%, last=00:08:5.171<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 40\u001B[0m\n\u001B[0;32m     37\u001B[0m         episode_rewards\u001B[38;5;241m.\u001B[39mappend(sum_reward)\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m \u001B[43moptimize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time()\n\u001B[0;32m     42\u001B[0m loading_bar(i_episode, num_episodes, end_time \u001B[38;5;241m-\u001B[39m start_time)\n",
      "Cell \u001B[1;32mIn[32], line 26\u001B[0m, in \u001B[0;36moptimize_model\u001B[1;34m()\u001B[0m\n\u001B[0;32m     22\u001B[0m     returns\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m, G)\n\u001B[0;32m     24\u001B[0m returns \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(returns, device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m---> 26\u001B[0m old_probs \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgather\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m     29\u001B[0m     new_props \u001B[38;5;241m=\u001B[39m policy_net(states)\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m1\u001B[39m, actions)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "policy_net = PolicyNetwork(train_env.observation_space.shape[0], train_env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(int(1e6))\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "num_episodes = 10000\n",
    "episode_rewards = []\n",
    "\n",
    "start_time = time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = train_env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    sum_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = train_env.step(action.item())\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = bool(terminated)\n",
    "        steps_done += 1\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        print(type(state))\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "        sum_reward += reward.item()\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(sum_reward)\n",
    "            break\n",
    "\n",
    "    optimize_model()\n",
    "    end_time = time()\n",
    "    loading_bar(i_episode, num_episodes, end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ae5d5e4c063cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = []\n",
    "\n",
    "for i in range(0, len(episode_rewards)):\n",
    "    # print(f\"Episode {i}, mean reward: {np.mean(episode_rewards[0:i])}\")\n",
    "    mean_rewards.append(np.mean(episode_rewards[0:i]))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_rewards)\n",
    "plt.plot(mean_rewards, color=\"red\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
